{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Intro\n",
    "\n",
    "This script is about getting morphosyntactic polish dictionary into some python-readable form and then lemmatizing and normalizing all songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need polish morphosyntactic dictionary - you can find one on (link needed) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAIN_PATH = \"/home/jack/datasets/polish_rap/\"\n",
    "DICTIONARIES_PATH = \"/home/jack/datasets/polish_dictionaries/\"\n",
    "\n",
    "MORPHOSYNTACTIC_DICT = \"slownik_morfosyntaktyczny.csv\"\n",
    "NORMALIZED_DICT = \"slownik_znormalizowany.json\"\n",
    "\n",
    "STRONG_LANGUAGE_LIST = \"strong_language.json\"\n",
    "STRONG_LANGUAGE_LEMMATIZED = \"strong_language_lem.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Normalizing morphosyntactic dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_normalized_dict(force=False):\n",
    "    if os.path.exists(DICTIONARIES_PATH + NORMALIZED_DICT) and not force:\n",
    "        with open(DICTIONARIES_PATH + NORMALIZED_DICT, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def lemmatize_df(word):\n",
    "        t = slownik_pl[slownik_pl[1] == word]\n",
    "        return t[0].to_string(index=False).split()[0] if not t.empty else word\n",
    "    \n",
    "    if not os.path.exists(DICTIONARIES_PATH + MORPHOSYNTACTIC_DICT):\n",
    "        raise Exception(\"There is no morphosyntactic dictionary in the path you specified.\")\n",
    "    \n",
    "    with open(DICTIONARIES_PATH + MORPHOSYNTACTIC_DICT, \"r\") as f:\n",
    "        slownik_pl = pd.read_csv(f, sep=\";\", header=None)\n",
    "\n",
    "    slownik_pl.drop(2, axis=1, inplace=True)\n",
    "    \n",
    "    # sanity checks\n",
    "    assert(slownik_pl[slownik_pl[1] == \"ul\"][0].to_string(index=False).split()[0] == \"ul\")\n",
    "    assert(lemmatize_df(\"poszukiwani\") == \"poszukiwany\")\n",
    "    \n",
    "    slownik_pl_dict = slownik_pl.set_index(1)[0].to_dict()\n",
    "    slownik_pl_dict = {normalize(k): normalize(v) for k, v in slownik_pl_dict.items()}\n",
    "    \n",
    "    with open(\"/home/jack/datasets/polish_dictionaries/slownik_znormalizowany.json\", \"w\") as f:\n",
    "        json.dump(slownik_pl_dict, f, ensure_ascii=False)\n",
    "    \n",
    "    return slownik_pl_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slownik_pl_dict = get_normalized_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize(dictionary, word):\n",
    "    return dictionary[word] if word in dictionary else word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_noise(word):\n",
    "    # function to remove noise, such as google adwords stuff...\n",
    "    if \"google\" in word or len(word) > 30:\n",
    "        return \"\"\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_letters = set(string.ascii_lowercase + \"ąćłóężźńś\")\n",
    "def normalize(word):\n",
    "    word = word.lower()\n",
    "    word = \"\".join(c for c in word if c in good_letters)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "assert(lemmatize(slownik_pl_dict, \"poszukiwani\") == \"poszukiwać\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zapalić'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(slownik_pl_dict, \"zapalisz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Lemmatizing all songs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optionally_mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatize_all(force=False):\n",
    "    rappers = os.listdir(MAIN_PATH + \"ok_lyrics/\")\n",
    "    optionally_mkdir(MAIN_PATH + \"stm_lyrics/\")\n",
    "\n",
    "    for rapper in tqdm(rappers):\n",
    "        songs = os.listdir(MAIN_PATH + \"ok_lyrics/\" + rapper)\n",
    "        optionally_mkdir(MAIN_PATH + \"stm_lyrics/\" + rapper)\n",
    "        for song in songs:\n",
    "            new_song = \"\"\n",
    "            if not os.path.exists(MAIN_PATH + \"stm_lyrics/\" + rapper + \"/\" + song) or force:\n",
    "                with open(MAIN_PATH + \"ok_lyrics/\" + rapper + \"/\" + song, \"r\") as f:\n",
    "                    old_song_json = json.load(f)\n",
    "                old_song = old_song_json[\"lyrics\"]\n",
    "                for line in old_song.split(\"\\n\"):\n",
    "                    for word in line.split(\" \"):\n",
    "                        word = normalize(word)\n",
    "                        word = lemmatize(slownik_pl_dict, word)\n",
    "                        word = remove_noise(word)\n",
    "                        new_song += word\n",
    "                        new_song += \" \"\n",
    "                    new_song += \"\\n\"\n",
    "                #warning - it leaves space at the end of each line and newline at the end of file\n",
    "                new_song_json = old_song_json\n",
    "                new_song_json[\"lyrics\"] = new_song\n",
    "                with open(MAIN_PATH + \"stm_lyrics/\" + rapper + \"/\" + song, \"w\") as f:\n",
    "                    old_song = json.dump(new_song_json, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:07<00:00, 10.85it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%timeit\n",
    "lemmatize_all(force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Lemmatizing dictionary of strong vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatize_strongs():\n",
    "    new_strongs = set()\n",
    "    with open(DICTIONARIES_PATH + STRONG_LANGUAGE_LIST, \"r\") as f:\n",
    "        strongs = json.load(f)\n",
    "    for strong in strongs:\n",
    "        new_strongs.add(lemmatize(slownik_pl_dict, strong))\n",
    "    new_strongs = list(new_strongs)\n",
    "\n",
    "    with open(DICTIONARIES_PATH + STRONG_LANGUAGE_LEMMATIZED, \"w\") as f:\n",
    "        json.dump(new_strongs, f, ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatize_strongs()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
